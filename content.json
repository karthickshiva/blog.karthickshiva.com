{"pages":[{"title":"About","text":"Welcome to my blog! My name is Karthick Shiva, and I am a software developer with a passion for programming and technology. I created this blog to share my knowledge and insights on various programming topics, including web development, software engineering, and mobile app development. Whether you’re a beginner or an experienced programmer, my goal is to provide you with valuable information and resources that will help you improve your skills and stay up-to-date with the latest trends in the industry. I believe that technology is constantly evolving, and it’s important to stay informed and adapt to new changes in order to succeed in this field. Through this blog, I hope to inspire and motivate others to pursue their passion for programming and embrace the endless possibilities that technology has to offer. Thank you for visiting my blog, and I hope you find the content informative and helpful. If you have any questions or suggestions for future topics, please feel free to reach out to me.","link":"/about.html"},{"title":"Articles","text":"","link":"/articles/index.html"}],"posts":[{"title":"Dependency Injection","text":"Dependency injection is a design pattern used in software engineering that allows objects to be created with their dependencies supplied from outside sources. In other words, instead of an object creating its dependencies itself, the dependencies are “injected” into the object from an external source. The main benefits of dependency injection are: Decoupling: By injecting dependencies, objects are not tightly coupled to their dependencies, which makes them more modular and easier to test. Testability: Because dependencies can be easily replaced with mock objects, unit testing becomes easier and more effective. Reusability: Injected dependencies can be reused across multiple objects, reducing code duplication and improving maintainability. There are three main types of dependency injection: Constructor Injection: Dependencies are passed to an object’s constructor when it is created. Setter Injection: Dependencies are set on an object using setter methods. Interface Injection: Objects are required to implement a specific interface that defines the dependencies they require. Dependency injection frameworks are available in many programming languages to help automate the process of injecting dependencies. These frameworks use a combination of reflection and configuration files to automatically inject dependencies into objects at runtime. Examples of dependency injection frameworks include Spring Framework for Java and AngularJS for JavaScript. Here are some examples of dependency injection in Java: Constructor Injection:1234567891011public class UserService { private final UserRepository userRepository; public UserService(UserRepository userRepository) { this.userRepository = userRepository; } public User getUserById(int id) { return userRepository.findById(id); }} In this example, the UserService class has a dependency on the UserRepository class, which is passed to its constructor. The UserRepository object is injected into the UserService object when it is created. This allows the UserService object to use the methods of the UserRepository object without creating it itself. Setter Injection:123456789101112public class OrderService { private PaymentService paymentService; public void setPaymentService(PaymentService paymentService) { this.paymentService = paymentService; } public void processOrder(Order order) { paymentService.processPayment(order.getPayment()); // process order logic }} In this example, the OrderService class has a dependency on the PaymentService class, which is set using a setter method. The PaymentService object is injected into the OrderService object after it is created. This allows the OrderService object to use the methods of the PaymentService object without creating it itself. Interface Injection:12345678910111213141516171819202122public interface Logger { void log(String message);}public class ConsoleLogger implements Logger { public void log(String message) { System.out.println(message); }}public class UserService { private Logger logger; public void setLogger(Logger logger) { this.logger = logger; } public User getUserById(int id) { logger.log(\"Getting user by id: \" + id); // get user logic }} In this example, the UserService class has a dependency on the Logger interface, which is required to be implemented by any object that wants to be injected into the UserService object. The ConsoleLogger class implements the Logger interface and is injected into the UserService object using a setter method. This allows the UserService object to use the methods of the Logger object without creating it itself. These are just a few examples of how dependency injection can be used in Java. There are many other ways to use dependency injection, and the specific implementation depends on the needs of the application. Let’s say we have a class called “UserService” that is responsible for managing user data. This class has a dependency on a database connection to retrieve and store user data. Without dependency injection, the UserService class would have to create its own database connection object, which would tightly couple the UserService class to the database connection implementation. With dependency injection, we can pass in the database connection object as a dependency to the UserService class. This allows us to easily swap out different database connection implementations without having to modify the UserService class. For example, we could create a MySQLDatabaseConnection class and a PostgreSQLDatabaseConnection class, both implementing the same interface. We can then pass in either of these objects to the UserService class depending on which database we want to use. This makes our code more flexible and easier to maintain, as we can easily swap out dependencies without having to modify the code that uses them.","link":"/2023/04/22/Dependency-Injection/"},{"title":"Implementing Unique ID Generator","text":"A unique ID generator can be implemented using a combination of timestamp, counter, and random number. Here’s a possible implementation in Python: 123456789101112import timeimport randomclass IDGenerator: def __init__(self): self.counter = 0 def generate_id(self): timestamp = int(time.time() * 1000) # Get current timestamp in milliseconds self.counter = (self.counter + 1) % 10000 # Increment counter and wrap around random_num = random.randint(0, 999) # Generate random number between 0 and 999 return f\"{timestamp:013d}{self.counter:04d}{random_num:03d}\" In this implementation, the IDGenerator class has a counter initialized to zero. The generate_id method generates a unique ID by concatenating the current timestamp in milliseconds, a counter value that increments with each ID generation, and a random number between 0 and 999. The resulting ID is a 20-digit string in the format timestamp (13 digits) + counter (4 digits) + random number (3 digits). Here’s an example of how to use the IDGenerator class: 123generator = IDGenerator()for i in range(10): print(generator.generate_id()) This code creates an instance of the IDGenerator class and generates 10 unique IDs using the generate_id method. The output might look like this: 12345678910162798978283100000000000162798978283100000000001162798978283100000000002162798978283100000000003162798978283100000000004162798978283100000000005162798978283100000000006162798978283100000000007162798978283100000000008162798978283100000000009 This implementation generates unique IDs by combining a timestamp, a counter, and a random number, which ensures that the probability of collisions is very low. However, it is possible for collisions to occur if the same ID generator is used across multiple machines or if the counter wraps around too quickly. To further optimize the ID generator, additional measures such as using a stronger hash function or a distributed ID generation system may be necessary. Other options:There are several options to generate unique IDs, depending on the requirements of the application. Here are some common methods: UUID: A UUID (Universally Unique Identifier) is a 128-bit number that is guaranteed to be unique across time and space. UUIDs are generated using a combination of timestamp and random number, and can be represented as a string of hexadecimal digits. Timestamp: A timestamp is a value that represents the current date and time. Timestamps can be used as unique IDs if they are combined with a counter or a random number to ensure uniqueness. Counter: A counter is a value that increments with each ID generation. Counters can be used as unique IDs if they are combined with a timestamp or a random number to ensure uniqueness. Hash function: A hash function can be used to generate a unique ID from a given input. Hash functions take an input of arbitrary size and produce a fixed-size output that is unique for each input value. Snowflake ID: A Snowflake ID is a unique 64-bit integer that is generated using a combination of timestamp, machine ID, and sequence number. Snowflake IDs are used by distributed systems to generate unique IDs across multiple machines. Custom ID: A custom ID can be generated using any combination of the above methods, or by using a custom algorithm that meets the specific requirements of the application. Each of these methods has its own advantages and disadvantages, and the choice of ID generation method depends on the specific requirements of the application. For example, UUIDs are widely used because they are guaranteed to be unique, but they are relatively long and may not be suitable for some applications. On the other hand, counters are simple and efficient, but they may not be unique if multiple ID generators are used simultaneously. Auto Incremented ID:Using auto-incremented IDs as primary keys in SQL databases is a common practice and works well in many cases. However, there are some situations where this approach may not be suitable: Scalability: If the database is expected to grow very large, auto-incremented IDs may eventually overflow the maximum value of the data type used to store them. This can cause errors and require expensive database migrations. Security: Auto-incremented IDs can be predictable, which can be a security risk if they are used in URLs or other public-facing contexts. Attackers can use this predictability to guess other IDs and access sensitive data. Data privacy: In some cases, auto-incremented IDs can reveal information about the data, such as the order in which it was added to the database. This can be a privacy concern if the data contains sensitive information. Data integration: If data from multiple databases needs to be merged or integrated, auto-incremented IDs may not be unique across the different databases, leading to conflicts and errors. For these reasons, it may be necessary to use other methods to generate unique IDs, such as UUIDs or custom ID generators. These methods can provide better scalability, security, and data privacy, and can be more suitable for distributed systems or applications with complex data integration requirements. UUID:UUIDs (Universally Unique Identifiers) are widely used as unique identifiers in many applications and have several advantages, such as being guaranteed to be unique and not requiring a centralized ID generator. However, there are some situations where UUIDs may not be suitable: Size: UUIDs are relatively long, typically 32 hexadecimal digits (128 bits). This can be a problem if the IDs need to be stored in a database or transmitted over a network, as it can increase storage and bandwidth requirements. Predictability: Although UUIDs are designed to be unique, they are not completely random and can be predictable in some cases. This can be a security risk if the UUIDs are used in URLs or other public-facing contexts. Performance: Generating UUIDs can be computationally expensive, especially if they are generated in large batches or in a distributed system. This can affect the performance of the application and increase response times. Integration: If data from multiple systems needs to be merged or integrated, UUIDs may not be unique across the different systems, leading to conflicts and errors. For these reasons, it may be necessary to use other methods to generate unique IDs, such as custom ID generators or other types of UUIDs (such as ULIDs or Flake IDs) that address some of the limitations of standard UUIDs. The choice of ID generation method depends on the specific requirements of the application and the trade-offs between uniqueness, size, predictability, and performance. Snowflake ID:Snowflake ID is a unique 64-bit integer that is generated using a combination of timestamp, machine ID, and sequence number. Snowflake IDs are used by distributed systems to generate unique IDs across multiple machines. Here’s an example of how Snowflake ID is generated: Let’s assume that we have a distributed system with multiple machines. Each machine has a unique ID, which is a 10-bit integer. The current timestamp in milliseconds is 41 bits long. The sequence number is a 12-bit integer that increments with each ID generation on the same machine. To generate a Snowflake ID, we can concatenate these three values into a 64-bit integer in the following order: The first 41 bits represent the current timestamp in milliseconds. The next 10 bits represent the machine ID. The last 12 bits represent the sequence number. Here’s an example of how a Snowflake ID might look like: 1110011001101001110101011110010011101001110000000000000000000000 (64 bits) In this example, the first 41 bits represent the timestamp, which is equivalent to the value 1630055836000 in milliseconds. The next 10 bits represent the machine ID, which could be any value between 0 and 1023. The last 12 bits represent the sequence number, which could be any value between 0 and 4095. By using a combination of timestamp, machine ID, and sequence number, Snowflake IDs can be generated with a high degree of uniqueness and can be used to generate IDs across multiple machines in a distributed system. Here’s an implementation of the Snowflake method for generating unique IDs in Python: 123456789101112131415161718192021222324252627282930313233343536373839import timeclass SnowflakeGenerator: def __init__(self, datacenter_id, worker_id): self.twepoch = 1288834974657 self.datacenter_id = datacenter_id self.worker_id = worker_id self.sequence = 0 self.sequence_bits = 12 self.worker_id_bits = 5 self.datacenter_id_bits = 5 self.max_worker_id = -1 ^ (-1 &lt;&lt; self.worker_id_bits) self.max_datacenter_id = -1 ^ (-1 &lt;&lt; self.datacenter_id_bits) self.sequence_mask = -1 ^ (-1 &lt;&lt; self.sequence_bits) self.worker_id_shift = self.sequence_bits self.datacenter_id_shift = self.sequence_bits + self.worker_id_bits self.timestamp_shift = self.sequence_bits + self.worker_id_bits + self.datacenter_id_bits def _generate_timestamp(self): return int(time.time() * 1000 - self.twepoch) def _next_sequence(self): self.sequence = (self.sequence + 1) &amp; self.sequence_mask if self.sequence == 0: raise Exception(\"Sequence overflow\") def generate_id(self): timestamp = self._generate_timestamp() if timestamp &lt; self.last_timestamp: raise Exception(\"Clock moved backwards\") if timestamp == self.last_timestamp: self._next_sequence() else: self.sequence = 0 self.last_timestamp = timestamp return ((timestamp &lt;&lt; self.timestamp_shift) | (self.datacenter_id &lt;&lt; self.datacenter_id_shift) | (self.worker_id &lt;&lt; self.worker_id_shift) | self.sequence) Here’s how you can use this class to generate unique IDs: 123generator = SnowflakeGenerator(datacenter_id=1, worker_id=1)unique_id = generator.generate_id()print(unique_id) This will output a unique ID that is generated using the Snowflake method. You can change the datacenter_id and worker_id values to generate IDs that are unique to your specific environment. The code is an implementation of the Snowflake method for generating unique IDs in Python. The SnowflakeGenerator class has the following attributes: twepoch: This is the timestamp of the Snowflake epoch, which is January 1, 2010 in milliseconds. It is used to calculate the timestamp portion of the generated ID. datacenter_id: This is a unique identifier for the datacenter that the ID is being generated in. worker_id: This is a unique identifier for the worker that is generating the ID. sequence: This is a counter that is used to ensure that IDs generated within the same millisecond are unique. sequence_bits: This is the number of bits used to represent the sequence number. worker_id_bits: This is the number of bits used to represent the worker ID. datacenter_id_bits: This is the number of bits used to represent the datacenter ID. max_worker_id: This is the maximum value that the worker ID can be. max_datacenter_id: This is the maximum value that the datacenter ID can be. sequence_mask: This is a bitmask that is used to extract the sequence number from the generated ID. worker_id_shift: This is the number of bits to shift the worker ID to the left before combining it with the other parts of the ID. datacenter_id_shift: This is the number of bits to shift the datacenter ID to the left before combining it with the other parts of the ID. timestamp_shift: This is the number of bits to shift the timestamp to the left before combining it with the other parts of the ID. The SnowflakeGenerator class has the following methods: _generate_timestamp(): This method generates the timestamp portion of the ID by subtracting the Snowflake epoch from the current time in milliseconds. _next_sequence(): This method increments the sequence number and handles sequence number overflow. generate_id(): This method generates a unique ID using the Snowflake method. It combines the timestamp, datacenter ID, worker ID, and sequence number to create a 64-bit ID. To use the SnowflakeGenerator class, you can create an instance of the class with a unique datacenter ID and worker ID, and then call the generate_id() method to generate a new ID. The generated ID will be unique to your specific environment. Note:The Snowflake method was originally developed by Twitter to generate unique IDs for their distributed systems. They chose January 1, 2010 as the epoch for their implementation of the Snowflake method because it was a recent date at the time and it allowed for a larger range of timestamps than if they had chosen an earlier epoch. In the Snowflake method, the timestamp portion of the ID is calculated by subtracting the epoch time from the current time in milliseconds. By choosing a more recent epoch time, the timestamp portion of the ID can be represented using fewer bits, which allows for a larger range of timestamps to be represented in the ID. It’s worth noting that the choice of epoch time is somewhat arbitrary and can be adjusted to suit the needs of a particular system. However, it’s important to choose an epoch time that allows for a sufficient range of timestamps to be represented in the ID, while also ensuring that the timestamp portion of the ID doesn’t take up too many bits and reduce the number of bits available for other parts of the ID (such as the worker ID and sequence number).","link":"/2023/04/22/Implementing-Unique-ID-Generator/"},{"title":"Using Threading to Print Odd and Even Numbers in Order","text":"In this blog post, we’ll explore how to use threading in Java to print odd and even numbers in order. Threading is a powerful technique that allows us to execute multiple threads of code concurrently, which can be useful for a wide variety of applications, including parallel processing, network programming, and more. In this example, we’ll use threading to print a sequence of odd and even numbers in order. We’ll start by creating two threads, one for printing odd numbers and one for printing even numbers. Each thread will execute a loop that prints the appropriate numbers, and we’ll use a synchronization primitive called a semaphore to ensure that the threads execute in the correct order. By the end of this blog post, you’ll have a better understanding of how to use threading in Java to execute concurrent tasks. We’re going to discuss about two major methods to implement this. Using synchronized method:In Java, the synchronized keyword is used to create synchronized methods, which are methods that can be accessed by only one thread at a time. When a thread invokes a synchronized method, it acquires a lock on the object that the method is called on, and no other thread can access the synchronized method on that object until the lock is released. Here’s a sample Java code that creates two separate threads to print odd and even numbers: 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class OddEvenPrinter { private final int MAX_VALUE = 10; private int currentValue = 1; public static void main(String[] args) { OddEvenPrinter printer = new OddEvenPrinter(); Thread oddThread = new Thread(printer::printOdd, \"Odd\"); Thread evenThread = new Thread(printer::printEven, \"Even\"); oddThread.start(); evenThread.start(); } public synchronized void printOdd() { while (currentValue &lt;= MAX_VALUE) { if (currentValue % 2 != 0) { System.out.println(Thread.currentThread().getName() + \": \" + currentValue); currentValue++; notify(); } else { try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } } public synchronized void printEven() { while (currentValue &lt;= MAX_VALUE) { if (currentValue % 2 == 0) { System.out.println(Thread.currentThread().getName() + \": \" + currentValue); currentValue++; notify(); } else { try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } }} This code should print the following output: 12345678910Odd: 1Even: 2Odd: 3Even: 4Odd: 5Even: 6Odd: 7Even: 8Odd: 9Even: 10 In this code, we define a OddEvenPrinter class that has a MAX_VALUE constant that specifies the maximum number to print, and a currentValue variable that keeps track of the current number to print. We create two separate threads, one for printing odd numbers and one for printing even numbers, using the Thread class and lambda expressions. The printOdd() and printEven() methods use a synchronized block to ensure that only one thread can access the shared currentValue variable at a time. Within each method, we use a while loop to print odd and even numbers, respectively. If the current value is odd and the current thread is the odd thread, we print the current value and increment the currentValue variable, and then notify the other thread to wake up. If the current value is even and the current thread is the even thread, we print the current value and increment the currentValue variable, and then notify the other thread to wake up. Otherwise, we wait for the other thread to notify us. By the end of this code, you should have two separate threads that print odd and even numbers in order. Using Semaphores:Semaphores are a synchronization mechanism that is used to control access to a shared resource in a concurrent system. They were first introduced by Edsger Dijkstra in 1965. A semaphore is essentially a counter that is associated with a shared resource. The counter can be incremented or decremented by threads that wish to access the shared resource. When the counter is greater than zero, the resource is available for use. When the counter is zero, the resource is unavailable and threads that wish to access it must wait until it becomes available. In Java, the Semaphore class is provided as part of the java.util.concurrent package. It provides methods for acquiring and releasing permits, which are equivalent to incrementing and decrementing the counter associated with the semaphore. Semaphores can be used in conjunction with other synchronization mechanisms such as locks and condition variables to implement more complex synchronization patterns. Here’s a sample Java code that uses Semaphores to print odd and even numbers in order: 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.concurrent.Semaphore;public class OddEvenPrinter { private final int MAX_VALUE = 10; private int currentValue = 1; private Semaphore oddSemaphore = new Semaphore(1); private Semaphore evenSemaphore = new Semaphore(0); public static void main(String[] args) { OddEvenPrinter printer = new OddEvenPrinter(); Thread oddThread = new Thread(printer::printOdd, \"Odd\"); Thread evenThread = new Thread(printer::printEven, \"Even\"); oddThread.start(); evenThread.start(); } public void printOdd() { while (currentValue &lt;= MAX_VALUE) { try { oddSemaphore.acquire(); System.out.println(Thread.currentThread().getName() + \": \" + currentValue); currentValue++; evenSemaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } } } public void printEven() { while (currentValue &lt;= MAX_VALUE) { try { evenSemaphore.acquire(); System.out.println(Thread.currentThread().getName() + \": \" + currentValue); currentValue++; oddSemaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } } }} In this code, we define a OddEvenPrinter class that has a MAX_VALUE constant that specifies the maximum number to print, and a currentValue variable that keeps track of the current number to print. We create two Semaphore objects, oddSemaphore and evenSemaphore, that are used to synchronize access to the shared currentValue variable. The oddSemaphore is initialized with a permit count of 1, and the evenSemaphore is initialized with a permit count of 0. We create two separate threads, one for printing odd numbers and one for printing even numbers, using the Thread class and lambda expressions. The printOdd() and printEven() methods use the acquire() and release() methods of the Semaphore class to ensure that only one thread can access the shared currentValue variable at a time. Within each method, we use a while loop to print odd and even numbers, respectively. If the current value is odd and the current thread is the odd thread, we acquire a permit from the oddSemaphore, print the current value, increment the currentValue variable, and release a permit to the evenSemaphore. If the current value is even and the current thread is the even thread, we acquire a permit from the evenSemaphore, print the current value, increment the currentValue variable, and release a permit to the oddSemaphore. By the end of this code, you should have two separate threads that print odd and even numbers in order using Semaphores. synchronized vs Semaphore:In terms of performance, Semaphores are generally faster and more efficient than synchronized methods for managing concurrency in Java. This is because Semaphores involve less overhead than synchronized methods, and they allow for more fine-grained control over access to shared resources. For this problem, both approaches (synchronized methods and Semaphores) are viable solutions and will produce correct results. However, since Semaphores are more efficient than synchronized methods, using Semaphores would be the better choice for this problem in terms of performance. In the Semaphore implementation of the Odd Even problem, we use two Semaphores to control access to the shared currentValue variable. The oddSemaphore is initialized with a permit count of 1, and the evenSemaphore is initialized with a permit count of 0. This allows us to ensure that only one thread can access the shared currentValue variable at a time, and that the threads take turns printing odd and even numbers. Overall, using Semaphores is a good choice for managing concurrency in Java applications, especially when performance is a concern. However, it’s important to note that Semaphores can be more difficult to use correctly than synchronized methods, and they require more careful design and testing to ensure that they work as intended.","link":"/2023/04/22/Using-Threading-to-Print-Odd-and-Even-Numbers-in-Order/"},{"title":"From Shipping Containers to Kubernetes: A Brief History of Containerization","text":"Containerization has come a long way since the days of shipping containers. In the world of technology, containerization has become a popular way to package and deploy applications. One of the most popular containerization platforms is Kubernetes. In this post, we’ll take a look at the history of containerization and how it has evolved to become the powerful platform that is Kubernetes. The Early Days of ContainerizationThe concept of containerization dates back to the 1950s, when shipping companies were looking for a way to transport goods more efficiently. The shipping industry developed standardized containers that could be easily loaded onto ships, trains, and trucks. This made it easier to transport goods across long distances and reduced the cost of shipping. In the 1970s, the concept of containerization was applied to the world of computing. The idea was to create a standardized way to package and deploy software applications. This would make it easier to move applications between different environments, such as development, testing, and production. The Rise of VirtualizationIn the 1990s, virtualization became a popular way to package and deploy applications. Virtualization allowed multiple applications to run on a single server, making it more efficient and cost-effective. However, virtualization had its drawbacks. It was resource-intensive and required a lot of overhead. The Birth of DockerIn 2013, Docker was introduced as a new way to package and deploy applications. Docker was built on top of the Linux container technology and provided a way to package applications in a lightweight and portable container. Docker quickly became popular and was adopted by many companies. The Emergence of KubernetesAs more companies started to use Docker, they realized that managing containers at scale was a challenge. This led to the development of Kubernetes, an open-source container orchestration platform. Kubernetes was designed to automate the deployment, scaling, and management of containerized applications. Kubernetes provides a way to manage and orchestrate containers across multiple hosts, making it easier to deploy and manage applications at scale. Kubernetes provides a declarative API that allows you to define the desired state of your application, and it takes care of the rest. Kubernetes ExplainedSuppose you have a web application that consists of multiple microservices. Each microservice is packaged in a Docker container and runs on a separate server. You want to deploy this application to a Kubernetes cluster and manage it using Kubernetes. First, you would create a Kubernetes deployment that defines the desired state of your application. The deployment would specify the number of replicas for each microservice, the Docker image to use, and any other configuration options. Next, you would create a Kubernetes service that exposes your application to the outside world. The service would provide a stable IP address and DNS name for your application, and it would load balance traffic across the replicas of each microservice. Once you have created the deployment and service, Kubernetes takes care of the rest. Kubernetes monitors the state of your application and ensures that the actual state matches the desired state. If a container fails, Kubernetes automatically restarts it. If a server goes down, Kubernetes automatically reschedules the containers on a different server. Kubernetes also provides a way to scale your application up or down based on demand. You can manually scale your application by updating the number of replicas in the deployment, or you can use Kubernetes’ autoscaling feature to automatically scale your application based on CPU usage, memory usage, or custom metrics. Kubernetes also provides a way to manage application updates and rollbacks. With Kubernetes, you can deploy new versions of your application without downtime. If something goes wrong, you can easily roll back to the previous version. Real Life Solutions with KubernetesKubernetes has become the de facto standard for container orchestration, and it’s used by many companies, including Airbnb, Spotify, and Lyft. These companies use Kubernetes to manage their applications at scale and provide a reliable and scalable service to their users. If you’re interested in using Kubernetes, there are many resources available to help you get started. The Kubernetes documentation is a great place to start, and there are many tutorials and courses available online. You can also use managed Kubernetes services like Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), or Microsoft Azure Kubernetes Service (AKS) to make it easier to deploy and manage your applications. Containerization has come a long way since the days of shipping containers. Kubernetes has become a powerful platform that can help businesses manage their applications at scale. If you’re looking to manage your applications more efficiently, Kubernetes is definitely worth considering.","link":"/2023/04/24/from-shipping-containers-to-kubernetes-a-brief-history-of-containerization/"},{"title":"Hello World","text":"Hello and welcome to my new blog! My name is Karthick Shiva and I’m a software developer with a passion for all things technical. I’ve been working in the field for several years now, and I’ve had the opportunity to work on a wide variety of subjects across different domains. I’ve always been interested in sharing my knowledge and experiences with others, which is why I decided to start this blog. Here, I plan to write about a range of technical topics that I find interesting, from programming languages and frameworks to software architecture and design patterns. My goal with this blog is to provide valuable insights and practical advice to fellow developers and anyone else who is interested in technology. Whether you’re a seasoned pro or just starting out, I hope you’ll find something useful and informative here. In addition to writing about technical topics, I also plan to share my thoughts on industry trends, news, and events. I believe that staying up-to-date on the latest developments in the field is essential for any developer, and I hope to provide a fresh perspective on the topics that matter most. So, if you’re interested in learning more about software development, technology, and the industry as a whole, I invite you to join me on this journey. I’m excited to share my knowledge and experiences with you, and I look forward to hearing your thoughts and feedback along the way. Thanks for reading, and stay tuned for more posts to come!","link":"/2023/04/22/hello-world/"},{"title":"Stock Market 101: Understanding the Basics for New Investors","text":"Imagine you’re a part-owner of a company. That’s essentially what happens when you buy a stock. A stock is a share of ownership in a company, and those shares are bought and sold on stock exchanges. It’s like being a co-owner in a business. Stock Exchanges and BrokersSo, to trade stocks, we use something called stock exchanges. These are platforms where buyers and sellers come together. Two well-known examples are the NSE (National Stock Exchange) and BSE (Bombay Stock Exchange). To actually make trades, you need a broker - think of them as the middlemen who execute your buy or sell orders. Market Orders and Limit OrdersWhen you’re ready to buy or sell a stock, you have a couple of options. A market order is like saying, “I’ll take whatever the current price is,” and it gets executed immediately. On the other hand, a limit order lets you set a specific price. For example, if you want to buy a stock but only if it’s under ₹50, you can set a limit order for ₹50 or less. Bulls and BearsNow, let’s talk about bulls and bears. No, not the animals! In the stock market, a “bull market” is when prices are rising, and people are generally optimistic. On the flip side, a “bear market” is when prices are falling, and people are more pessimistic. Research and AnalysisNow, let’s dive into how you can make informed decisions. There are two main types of analysis: fundamental and technical. Fundamental analysis involves looking at a company's financial health, earnings, and overall business to determine its value. Technical analysis is about studying historical price and volume data to predict future price movements. Fundamental analysis focuses on examining a company’s financial statements, earnings reports, management team, competitive position, and other qualitative and quantitative factors to assess its intrinsic value. It’s more about understanding the company’s business fundamentals rather than analyzing price movements. Technical analysis, on the other hand, is the approach that relies on charts and patterns to forecast future price movements. DiversificationDiversification is like not putting all your eggs in one basket. Instead of investing all your money in one stock, you spread it across different stocks or even different sectors. Risk ManagementManaging the risks associated with trading is essential. Setting stop-loss orders is a smart move. A stop-loss order is like a safety net – you decide the maximum amount you’re willing to lose on a trade, and if the stock hits that price, the broker automatically sells it to limit your losses. DividendsSome companies share a portion of their profits with shareholders in the form of dividends. It’s like getting a little bonus for being a shareholder. Dividend-paying stocks provide a regular stream of income for investors. It’s like earning a reward for holding onto shares in a company. Long-Term vs. Short-Term InvestingThere are two main approaches to investing: long-term and short-term. Long-term investors focus on the potential growth of their investments over several years, while short-term traders aim to profit from price fluctuations over shorter time frames. Long-term investing aligns well with goals like saving for retirement, allowing you to benefit from the potential growth of your investments over time. Market IndexMarket indices are like benchmarks that represent the overall performance of the stock market. The BSE and NSE are examples. They give you a sense of how the market as a whole is doing. Market indices provide an overview of the overall market health rather than individual companies. They give investors a sense of how the broader market is performing. Investors often use indices to gauge the general trend and sentiment in the market. NSE and BSE:Imagine these as the marketplaces where stocks are bought and sold. NSE is based in Mumbai, and BSE is, too. They’re like the heart of the Indian stock market. Think of NSE and BSE as big meeting places where buyers and sellers come together. Companies get their stocks listed on these exchanges, and investors like you and me trade these stocks through brokers on these platforms. BSE is one of the oldest stock exchanges globally, and NSE is a bit younger but has grown rapidly. Both are vital for India’s financial landscape. Market indices are like scorecards for the stock market. They help us understand how the market as a whole is doing. The most common indices in India are the Sensex (BSE) and the Nifty (NSE). The Sensex is like the big brother; it represents the 30 largest and most actively traded stocks on the BSE. The Nifty, on the other hand, is like the cool cousin, comprising 50 stocks from the NSE. How are they calculated?Think of it like a basket of stocks. The value of the index is calculated based on the weighted average of the prices of these stocks. So, when these 30 or 50 companies do well, the index goes up, and when they don’t, it goes down. The tricky part is that not all stocks are equal in these indices. The bigger companies have more influence. It’s like saying the opinion of the CEO matters more than the intern in a big meeting. So, if a big company’s stock price changes a lot, it affects the index more than if a smaller company’s stock does the same. That’s why they call it a “weighted” average. Assume we have three companies: A, B, and C. Company A has 100 shares outstanding, and its stock price is ₹50. Company B has 150 shares outstanding, and its stock price is ₹75. Company C has 200 shares outstanding, and its stock price is ₹100. Now, let’s calculate the market value and the index for both Nifty and Sensex. Nifty Calculation: Nifty includes 50 stocks, so let’s assume for simplicity we’re only including A and B. The total market value for A is ₹50 * 100 = ₹5000. The total market value for B is ₹75 * 150 = ₹11250. Add these together: ₹5000 + ₹11250 = ₹16250. Nifty is calculated as ₹16250 / 2 = ₹8125. Sensex Calculation: Sensex includes all three companies. The total market value for A is ₹50 * 100 = ₹5000. The total market value for B is ₹75 * 150 = ₹11250. The total market value for C is ₹100 * 200 = ₹20000. Add these together: ₹5000 + ₹11250 + ₹20000 = ₹36250. Sensex is calculated as ₹36250 / 3 = ₹12083.33. In real scenarios, these calculations involve many more companies and are adjusted for various factors like stock splits, corporate actions, and free-float market capitalization. So, when you hear that Nifty or Sensex has gone up or down, it’s reflecting the average movement of these selected stocks based on their market values. Remember, the actual calculations are more complex, but this simplified example gives you an idea of the process. If you have specific questions or need further clarification, feel free to ask! TaxesLastly, let’s touch on taxes. It’s essential to understand the tax implications of your trades and investment gains. Different types of investments may have different tax treatments.","link":"/2024/02/02/stock-market-basics/"},{"title":"Stock Market 101: Stock Valuation Methods","text":"Understanding how to value a stock is crucial for making informed investment decisions. There are various methods for stock valuation, and two common approaches are: Dividend Discount Model (DDM): This method values a stock based on the present value of its future dividend payments. It’s often used for companies that pay consistent dividends. Price/Earnings (P/E) Ratio: This ratio compares a company’s stock price to its earnings per share. It gives an indication of how much investors are willing to pay for each dollar of earnings. Let’s dive into the details of both the Dividend Discount Model (DDM) and the Price/Earnings (P/E) Ratio. Dividend Discount Model (DDM):The Dividend Discount Model is a valuation method that calculates the intrinsic value of a stock based on the present value of its expected future dividend payments. Here’s the formula: $$ \\text{Intrinsic Value} = \\frac{\\text{Dividend per Share}}{\\text{Discount Rate} - \\text{Growth Rate}} $$ Dividend per Share: The annual dividend payment expected per share. Discount Rate: The rate of return investors require to invest in the stock. Growth Rate: The expected rate at which dividends will grow. If a stock pays a steady dividend and you expect it to continue, the DDM helps estimate its fair value. Keep in mind that this model works best for stable dividend-paying companies. Deciding whether to buy, hold, or sell a stock using the Dividend Discount Model (DDM) involves assessing the intrinsic value calculated by the model and comparing it to the current market price. Here’s a step-by-step guide on how to use DDM for decision-making: Calculate Intrinsic Value: Use the DDM formula to calculate the intrinsic value of the stock. The formula is: $$ \\text{Intrinsic Value} = \\frac{\\text{Dividend per Share}}{\\text{Discount Rate} - \\text{Growth Rate}} $$ Input the expected future dividend payments, your required rate of return (discount rate), and the anticipated dividend growth rate. Compare Intrinsic Value to Market Price: Compare the calculated intrinsic value to the current market price of the stock. If the intrinsic value is higher than the market price, it may indicate that the stock is undervalued, and it could be a buying opportunity. If the intrinsic value is lower than the market price, it may suggest that the stock is overvalued, and you might consider holding or selling. Consider the Margin of Safety: Assess the margin of safety by evaluating how much the intrinsic value exceeds the current market price. A larger margin of safety provides a cushion against potential errors in estimating growth rates or discount rates. Monitor Changes in Fundamentals: Regularly monitor changes in the company’s fundamentals, such as dividend policies, earnings, and financial health. If these factors change significantly, it may impact the accuracy of your DDM calculations. Evaluate Economic and Market Conditions: Consider broader economic and market conditions. Economic changes, interest rate movements, or industry trends can affect discount rates and growth expectations, influencing the stock’s intrinsic value. Reassess Growth and Discount Rate Assumptions: Periodically reassess and update your assumptions for the growth rate and discount rate. Changes in the company’s prospects or market conditions may require adjustments to these inputs. Diversify Your Portfolio: Diversify your portfolio to manage risk. Relying solely on the DDM for decision-making may expose you to errors in estimating growth rates or discount rates. Risk Tolerance and Investment Goals: Consider your risk tolerance and investment goals. Stocks with higher growth rates and lower discount rates may be riskier but could offer higher potential returns. Remember, the DDM is just one valuation model, and its effectiveness depends on the accuracy of your assumptions. It’s essential to complement DDM with other fundamental and technical analysis tools and to stay informed about the overall market and economic conditions. Price/Earnings (P/E) Ratio:The Price/Earnings ratio is a widely used metric for stock valuation. It compares the current market price of a stock to its earnings per share (EPS). The formula is: $$ \\text{P/E Ratio} = \\frac{\\text{Market Price per Share}}{\\text{Earnings per Share (EPS)}} $$ Market Price per Share: The current price of one share of the stock. Earnings per Share (EPS): The company’s net earnings divided by the number of outstanding shares. The P/E ratio helps you understand how much investors are willing to pay for each dollar of earnings generated by the company. A high P/E ratio might suggest that investors have high expectations for future growth, while a low P/E ratio might indicate undervaluation or lower growth expectations. The decision to buy, hold, or sell a stock using the Price/Earnings (P/E) ratio involves assessing the current P/E ratio in the context of the company’s fundamentals, industry benchmarks, and your investment goals. Here’s a guide on how to use the P/E ratio for decision-making: High P/E Ratio (Overvalued): Buy Decision: A high P/E ratio might suggest that the market has high expectations for future earnings growth. If you believe the company can meet or exceed these expectations, it might still be a good buy. Hold/Sell Decision: However, if you think the market expectations are too optimistic or if the company’s growth prospects are uncertain, you might consider holding or selling the stock. Low P/E Ratio (Undervalued): Buy Decision: A low P/E ratio could indicate that the stock is undervalued relative to its earnings. If you believe the company’s fundamentals are strong and the low P/E is unjustified, it might be a buying opportunity. Hold/Sell Decision: On the other hand, if the low P/E is reflective of real concerns about the company’s future growth or financial health, you might consider holding or selling. Comparisons with Industry and Peers: Compare the P/E ratio of the stock with industry averages and the P/E ratios of similar companies. A stock trading at a significantly higher or lower P/E than its peers might warrant further investigation. Historical P/E Trends: Evaluate the stock’s historical P/E ratios. If the current P/E is significantly higher or lower than the historical average, it could indicate a change in market sentiment or expectations. Consider Growth Prospects: Understand the company’s growth prospects. A high P/E may be justified if the company is in a high-growth phase, while a low P/E may be appropriate for a mature company with stable earnings. Risk Tolerance and Investment Goals: Assess your risk tolerance and investment goals. High-growth stocks with high P/E ratios may offer greater potential returns but also come with higher risk. Lower P/E stocks may be more stable but could have slower growth. Remember, the P/E ratio is just one tool in the investor’s toolbox. It should be used in conjunction with other fundamental and technical analysis tools to make well-informed investment decisions. Additionally, regular monitoring of the company’s financial health and market conditions is essential for ongoing decision-making.","link":"/2024/02/03/stock-valuation-methods/"},{"title":"Unleashing the Cloud: A Historical Odyssey of Cloud Storage","text":"In the ever-evolving landscape of technology, few innovations have revolutionized the way we store and manage data like the advent of cloud storage. The history of cloud storage is a fascinating journey that spans several decades, marked by groundbreaking developments, technological leaps, and the relentless pursuit of efficiency. Let’s embark on a historical odyssey to explore the evolution of cloud storage. The Early Days - Inception of a Digital RevolutionIn the not-so-distant past, during an era when computers were colossal and the internet was in its infancy, visionary minds began conceiving a concept that would reshape the landscape of data storage and sharing. This marks the commencement of a profound journey that has significantly influenced the way we manage and exchange digital information. Picture a time when computers communicated like discreet agents, sending encrypted messages to one another through concealed channels. These messages were akin to coded letters, and deciphering them required a close physical proximity between the communicating computers. However, the question arose: What if these computing entities could transcend spatial constraints and share information remotely? This was the nascent idea that would eventually evolve into the concept of cloud storage. In the early days, the notion of cloud storage was akin to an unwritten narrative awaiting its chapters. The aspiration was to establish a distinctive realm, a virtual haven where computers could transfer data irrespective of geographic limitations. It was an aspiration to liberate information from the shackles of physical space, allowing it to traverse freely, much like the wind carries whispers across expansive landscapes. The trailblazers of cloud storage during this epoch were akin to technological sorcerers wielding colossal machines. They commenced the arduous task of mapping out the uncharted territories of these digital heavens. The vision was to create an environment where individuals, regardless of their location, could access a communal space to safeguard their digital assets securely. The dream transcended the mere exchange of data between computers; it envisioned a space where people could archive memories, cultivate ideas, and preserve creations. It was a glimpse into a future where the restrictions of physical storage would dissolve, ushering in a new era of limitless possibilities for creativity and collaboration. As the dream of cloud storage began taking tangible form, the pioneers encountered a labyrinth of challenges and enigmas. How could they materialize this vision? How could they construct a space that was not only secure but also universally accessible? These were the intricate questions that fueled their intellectual curiosity and determination. Little did they foresee that this dream would metamorphose into reality, fundamentally altering the dynamics of our digital existence. The early days of cloud storage were analogous to the opening chapter of a sweeping saga, with numerous compelling chapters poised to unfold. The journey had just commenced, and the allure of a virtual sanctuary in the clouds held the promise of captivating minds for generations to come. Dot-Com Magic - Unveiling the PotentialFast forward to the late 1990s, a period marked by the Dot-Com Boom, where the internet was transforming into a global phenomenon. During this epoch, the idea of cloud storage began to gain momentum, and tech visionaries set out to harness the power of distributed computing on an unprecedented scale. The world was witnessing the rise of the internet, a virtual realm where possibilities seemed endless. Against this backdrop, brilliant minds started contemplating how they could leverage this expansive network to enable computers to communicate and collaborate more seamlessly. The question that lingered was, “What if the internet could serve as a conduit for computers to exchange information more efficiently than ever before?” Companies and innovators began experimenting with a novel concept known as Software as a Service (SaaS). In 1999, Salesforce emerged as a trailblazer in this arena, introducing a paradigm shift in how software applications were delivered and accessed. This marked the dawn of a new era, where businesses could utilize applications over the internet rather than relying on traditional local installations. This era of Dot-Com Magic laid the groundwork for the transformative potential of the cloud. The visionaries behind this movement envisioned a future where computing resources could be distributed, shared, and accessed remotely. The concept of cloud computing was taking shape, and the prospect of a digital landscape unbound by physical constraints was becoming increasingly tangible. During this time, the dream of cloud storage expanded beyond the confines of a select few. It became a democratized vision, with the goal of making these advanced computing capabilities accessible to businesses and individuals alike. The cloud was no longer just a concept; it was becoming a powerful force that promised to reshape the way we interacted with technology. The Dot-Com Boom set the stage for the unveiling of the cloud’s potential. Companies were exploring ways to make computing resources more scalable, efficient, and cost-effective. The idea of a shared space in the digital realm, where data and applications could transcend geographical boundaries, was gaining momentum, paving the way for the next chapter in the saga of cloud storage. Little did the world know that this Dot-Com Magic was merely the prelude to a digital revolution that would redefine the landscape of computing and communication. As the curtain rose on the next act, the stage was set for even more incredible innovations, shaping the trajectory of cloud storage for years to come. Storage Wizards Arrive - Unleashing the Power of the CloudAs we venture into the early 2000s, the narrative of cloud storage takes a significant turn with the arrival of storage wizards who dared to bring the mystical concept into tangible reality. Among these wizards, one company stood out as a pioneer, wielding its magic to transform the way we store and manage data — Amazon. In 2006, Amazon introduced the world to its groundbreaking creation: Amazon S3, the Simple Storage Service. This marked a pivotal moment in the history of cloud storage. Amazon S3 was more than just a service; it was a digital treasure chest in the cloud, offering scalable and affordable storage solutions. Businesses and individuals no longer needed to grapple with the challenges of maintaining extensive physical infrastructure to store their digital assets. Amazon’s foray into cloud storage demonstrated the scalability and efficiency that the cloud could offer. It was no longer a distant dream; the cloud was becoming a practical solution for storing vast amounts of data securely. The storage wizards had successfully conjured a service that opened the doors for businesses of all sizes to leverage the power of the cloud. The magic of Amazon S3 didn’t just lie in its ability to store data. It introduced a new paradigm where businesses could access computing resources on-demand. This laid the foundation for the broader concept of Infrastructure as a Service (IaaS), allowing users to rent virtualized computing resources over the internet. The cloud was evolving into a dynamic ecosystem that empowered businesses to scale their operations without the constraints of physical infrastructure. The impact of Amazon S3 reverberated across industries, sparking interest and curiosity. Other companies, inspired by Amazon’s success, began exploring their own magical contributions to the world of cloud storage. The stage was set for a wave of innovation, as more storage wizards stepped forward to showcase their prowess in the burgeoning realm of cloud computing. The Cloud for Everyone - A Digital Playground EmergesVenturing further into the cloud storage chronicles, our narrative takes an exciting turn towards inclusivity and accessibility. Enter the era of the mid-2000s, where cloud storage was no longer the exclusive domain of businesses and tech aficionados but became a digital playground accessible to everyone. During this period, companies envisioned a world where individuals, regardless of technical expertise, could harness the benefits of cloud storage. Two notable players in this democratization of the cloud were Dropbox and Google, each bringing their unique magic to the digital realm. Founded in 2007, Dropbox became a trailblazer by introducing a user-friendly interface that made storing and sharing files a breeze. The concept of a virtual folder that seamlessly synchronized across devices resonated with users, transforming cloud storage into a tool for personal use. It was no longer just about businesses; it was about individuals sharing photos, documents, and memories effortlessly. Not far behind, Google, known for its innovative prowess, ventured into the cloud arena with the introduction of Google Drive in 2012. Google Drive was not merely a storage solution; it was a collaborative platform where users could create, edit, and share documents in real-time. The cloud was evolving into a space where ideas could be cultivated, and creativity could flourish, irrespective of one’s technical prowess. This shift towards user-friendly interfaces and intuitive features marked a significant departure from the complex world of early cloud computing. Cloud storage was no longer a distant concept; it became an integral part of people’s digital lives. Whether it was students collaborating on projects, families sharing photos, or friends planning events, the cloud had transformed into a digital haven for personal and collective experiences. The cloud for everyone meant that individuals could enjoy the benefits of seamless collaboration, data synchronization, and easy access from anywhere with an internet connection. The once-mystical concept of cloud storage had evolved into a practical and indispensable tool, making digital life more connected and dynamic. The magic of Dropbox and Google had opened the floodgates, paving the way for a future where the cloud would become an integral part of how we manage and share our digital experiences. Little did we know that even more exciting developments awaited, as the story of cloud storage continued to unfold. Mobile Integration - The Cloud Goes EverywhereVenturing into the late 2000s and early 2010s, our cloud storage journey takes an intriguing turn with the proliferation of smartphones and tablets. This era marks the seamless integration of cloud storage into our mobile lives, reshaping how we engage with our digital worlds. The cloud, once confined to desktops and laptops, now extends its reach to mobile devices. Companies like Apple and Google play pivotal roles in this transformation, influencing how we interact with our data on-the-go. In 2011, Apple unveiled iCloud, a service seamlessly connecting Apple devices and allowing users to store photos, documents, and app data in the cloud. Suddenly, the photos captured on our iPhones effortlessly synchronized with our iPads, creating a cohesive digital experience across Apple devices. iCloud becomes the invisible thread linking our digital lives, ensuring that our data is accessible wherever we go. Around the same time, Google expanded its cloud capabilities with the integration of Google Drive. This not only enables Android users to store and access files in the cloud but also facilitates collaboration on the go. Drafting a document on a smartphone or making edits on a tablet, the cloud transforms into a dynamic workspace that transcends the limitations of physical devices. The integration of cloud storage into mobile devices marks a paradigm shift in how we perceive and utilize digital information. No longer tethered to a specific computer, we can capture, create, and share experiences from virtually anywhere. The cloud becomes our portable companion, ensuring that our digital treasures are just a tap away. This chapter in our cloud storage saga reflects the evolving nature of technology, where mobility and accessibility become paramount. The cloud, no longer a distant server, becomes a dynamic force accompanying us throughout our daily lives. Little did we know that this integration was just the beginning, setting the stage for even more interconnected and mobile-friendly cloud experiences. As our devices become smarter, the cloud continues to weave its magic, making our digital worlds more interconnected and accessible than ever before. The Era of Security and ComplianceNavigating into the 2010s, a critical theme emerges in the unfolding narrative of cloud storage — the paramount importance of security and regulatory compliance. As businesses and individuals entrusted more sensitive information to the cloud, concerns about data protection and adherence to industry regulations took center stage. In response to these concerns, cloud service providers intensified their efforts to fortify the security of their platforms. Encryption technologies, both in transit and at rest, became standard features. This cryptographic magic ensured that even if unauthorized eyes attempted to intercept data during transmission or gain access to stored information, they would be met with an indecipherable code. Simultaneously, compliance with industry regulations and data protection laws became a top priority. Cloud service providers undertook the responsibility of ensuring that their platforms adhered to the strictest standards. This was particularly crucial for businesses operating in sectors with stringent regulatory requirements, such as healthcare and finance. The introduction of hybrid and multi-cloud solutions emerged as another enchanting development during this era. These solutions provided businesses with the flexibility to store sensitive data on private, on-premises servers, while still harnessing the benefits of the cloud for less sensitive information. It was a delicate balancing act, allowing organizations to maintain control over critical data while leveraging the scalability and efficiency of the cloud. The cloud, once seen as a nebulous space, became a fortress of digital trust. Users, whether individuals storing personal files or enterprises safeguarding proprietary information, now had greater confidence in the security measures implemented by cloud service providers. The cloud had evolved from a repository of convenience to a stronghold of data integrity. The Modern Landscape - Unleashing Advanced PossibilitiesVenturing into the contemporary landscape of cloud storage, we find ourselves amidst a realm of unprecedented possibilities and cutting-edge innovations. The narrative has transcended from mere storage solutions to an intricate tapestry woven with advanced technologies, reshaping the very fabric of how we interact with and leverage the cloud. One of the standout features of this era is the integration of digital and cryptocurrencies into the cloud storage ecosystem. Blockchain technology, the backbone of cryptocurrencies, is now making waves in ensuring the security and transparency of data transactions. This decentralized approach promises to revolutionize the way we perceive data integrity and access control in the cloud. The emergence of serverless architectures is another spellbinding development. With serverless computing, users no longer need to manage servers. Instead, they can focus on writing code, and the cloud provider automatically handles the infrastructure. This magical shift not only enhances efficiency but also opens doors to new realms of scalability and cost-effectiveness. Artificial Intelligence (AI) has also stepped onto the cloud stage, bringing with it a host of transformative capabilities. Machine learning algorithms, powered by vast amounts of cloud-stored data, enable systems to learn and adapt without explicit programming. This dynamic duo of AI and the cloud is revolutionizing data analysis, automation, and decision-making processes across various industries. Edge computing, yet another entrancing development, is bringing the cloud closer to the users. With edge computing, data processing occurs near the source of data generation, reducing latency and enabling real-time responses. This is particularly significant in the age of the Internet of Things (IoT), where devices are constantly generating data that requires immediate and localized processing. The modern landscape of cloud storage is not just about storing and retrieving data; it’s about unlocking the full potential of digital transformation. The cloud has evolved into an ecosystem where data is not just stored but harnessed, analyzed, and utilized to propel innovation and efficiency.","link":"/2024/01/28/unleashing-the-cloud-a-historical-odyssey-of-cloud-storage/"},{"title":"Collision, Randomization and Welzl's Algorithm","text":"Have you ever played a 3D video game and wondered how the game engine detects collisions between objects? I recently found myself thinking about this while playing a popular racing game. As I was racing my car around the track, I couldn’t help but wonder how the game engine was able to detect collisions between my car and the other cars on the track. After doing some research, I learned that one of the ways that game engines detect collisions is by using the minimum enclosing sphere algorithm. This algorithm is used to find the smallest sphere that encloses a set of points in 3D space, which is useful in detecting collisions between objects. In the game I was playing, each car was represented by a 3D model, which was made up of a large number of points in 3D space. The game engine used the minimum enclosing sphere algorithm to calculate the minimum enclosing sphere of each car’s 3D model. By doing this, the game engine was able to determine if the minimum enclosing spheres of two cars intersected, indicating a collision. I was fascinated by this and started thinking about other applications of the minimum enclosing sphere algorithm. I realized that it could be used in various real-life simulations, such as simulations of fluid dynamics and molecular dynamics. In these simulations, the algorithm could be used to detect collisions between particles or fluid elements, which could help to simulate the behavior of fluids or molecules in real-world applications. Minimum Enclosing Circle: The minimum enclosing circle problem involves finding the smallest circle that encloses a set of points in a 2D plane. This problem can be solved using a similar approach to the minimum enclosing sphere algorithm, but with some modifications to account for the reduced dimensionality. Feeling inspired by the idea of the minimum enclosing circle problem, I decided to try and solve it myself. I thought to myself, “how hard could it be to find the smallest circle that encloses a set of points in a 2D plane?” I started by sketching out some points on a piece of paper and drawing circles around them. I quickly realized that this was not going to work, as it was difficult to determine which circle was the smallest. I needed a more systematic approach. Next, I tried to come up with a naive solution. I thought about selecting two points from the set of points and finding the distance between them. I could then draw a circle with a radius equal to half the distance between the two points. This circle would definitely enclose the two points, but it may not enclose all the other points in the set. I then thought about selecting a third point and finding the circle that passes through all three points. This circle would definitely enclose the three points, but it may not be the smallest circle that encloses all the points in the set. I decided to try a brute force algorithm to solve the minimum enclosing circle problem. While brute force algorithms are not always the most efficient, they can be useful in certain situations, especially when dealing with small sets of points. My brute force algorithm involved checking every possible circle that could be drawn around the set of points. I started by selecting three points from the set and finding the smallest circle that encloses them. I then added one point at a time and checked if the circle still enclosed all the points. If it did, then I continued adding points until all the points in the set were included. If not, then I discarded the circle and started again with a different set of three points. While this algorithm was not the most efficient, it was a valid solution to the problem. It guaranteed that the smallest possible circle that encloses all the points in the set would be found. However, it was not practical for larger sets of points, as the number of possible circles to check would become prohibitively large. Welzl’s algorithm:Since my brute force algorithm was not practical for larger sets of points, I did some research to see if there were more efficient algorithms available for solving the minimum enclosing circle problem. To my surprise, I discovered that the problem had once been a hot research topic and that many algorithms had been developed to solve it. As I delved deeper into the research, I discovered that one of the most popular algorithms for solving the minimum enclosing circle problem was the Welzl’s algorithm. This algorithm involved selecting points one at a time and finding the minimum enclosing circle that included all the previously selected points. The Welzl’s algorithm was much more efficient than my brute force algorithm and could handle larger sets of points. It was also more accurate, as it guaranteed that the smallest possible circle that encloses all the points in the set would be found. The algorithm goes as follows: 123456789101112131415Algorithm: Welzl's Algorithm for Minimum Enclosing CircleInput: A set of n points P in a 2D planeOutput: The minimum enclosing circle that encloses all the points in P1. if |P| = 1, return a circle with radius 0 centered at the only point in P.2. if |P| = 2, return the circle with diameter defined by the two points in P.3. Randomly shuffle the points in P.4. Let R be the set of points chosen so far.5. Let D be the minimum enclosing circle that encloses the points in R.6. For each point p in P \\ R: a. If p is inside D, continue to the next point. b. Add p to R. c. Recursively call the algorithm on the set of points R and update D.7. Return the minimum enclosing circle D that encloses all the points in P. Randomization:Before diving into the implementation of the Welzl’s algorithm, it’s worth noting the importance of random shuffling in the algorithm. The algorithm shuffles the points randomly before selecting them, which makes the algorithm randomized. Randomness is a pretty cool concept, don’t you think? It’s like the universe is playing dice with us and we’re just trying to figure out the rules of the game. But did you know that randomness can actually help us optimize problems? It’s true! By shuffling the points randomly, the Welzl’s algorithm is able to explore different orders of points and find the one that leads to the most efficient and accurate solution. This is especially important when dealing with large sets of points, where the number of possible orders is large and the algorithm can benefit from the added randomness. Did you also know that randomness can be used to optimize other algorithms too? For example, Monte Carlo simulations use random numbers to simulate the behavior of a system and estimate the probability of certain outcomes. This is super useful in fields like finance, engineering, and physics, where it’s hard to simulate a system using deterministic algorithms. Randomness is even used in sorting algorithms, like randomized quicksort, to avoid worst-case scenarios. And in cryptography, randomized primality testing is used to determine whether a given number is prime or composite. It’s amazing how randomness can be applied in so many different ways to make algorithms more efficient and accurate. It just goes to show that sometimes we need to embrace the chaos to find the best solutions. Implementation:After learning about the Welzl’s algorithm for minimum enclosing circle, I was excited to implement it and see it in action. I decided to create a mini project to demonstrate the algorithm and its various applications. Despite the challenges, I was determined to implement the Welzl’s algorithm and see it in action. I started by writing the pseudo code in paper and then translated it into JavaScript code. I created a simple GUI that allowed the user to input a set of points and see the minimum enclosing circle that encloses all the points. After an hour of coding and debugging, I finally had a working implementation of the Welzl’s algorithm. I was amazed at how quickly the algorithm was able to find the minimum enclosing circle for even large sets of points. Midpoint and circumcenter:In addition to the algorithm, we may need to revisit our high school math. Let’s revisit some high school math concepts and discuss how to find a circle that passes through two points and three points. Finding a circle that passes through two points is a relatively simple process. We can use the midpoint of the line segment connecting the two points as the center of the circle, and the distance between the two points as the radius of the circle. This circle will pass through both points. To find the midpoint of the line segment connecting two points, we can use the following formula: $$ \\text{midpoint} = \\left(\\frac{x_1 + x_2}{2}, \\frac{y_1 + y_2}{2}\\right) $$ where $(x_1, y_1)$ and $(x_2, y_2)$ are the coordinates of the two points that define the line segment. Once we have the midpoint, we can find the distance between the two points using the distance formula: $$ \\text{distance} = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$ where $(x_1, y_1)$ and $(x_2, y_2)$ are the coordinates of the two points. The radius of the circle will be equal to half the distance between the two points, and the center of the circle will be the midpoint of the line segment connecting the two points. To begin with, finding a circle that passes through three points is a bit more involved than finding a circle that passes through two points. One way to do this is to use the fact that the perpendicular bisectors of the three line segments connecting the three points will intersect at the center of the circle. We can then use the distance between the center and any one of the points as the radius of the circle. Another approach to finding the circle that passes through three points is to use the inverse determinant and edge length approach. This method involves finding the circumcenter of the triangle formed by the three points and then using the distance between the circumcenter and any one of the points as the radius of the circle. To find the circumcenter of the triangle formed by the three points, we can use the following formula: $$x = \\frac{a^2(b^2 + c^2 - a^2)x_1 + b^2(a^2 + c^2 - b^2)x_2 + c^2(a^2 + b^2 - c^2)x_3}{2(a^2(b^2 + c^2 - a^2) + b^2(a^2 + c^2 - b^2) + c^2(a^2 + b^2 - c^2))}$$ $$y = \\frac{a^2(b^2 + c^2 - a^2)y_1 + b^2(a^2 + c^2 - b^2)y_2 + c^2(a^2 + b^2 - c^2)y_3}{2(a^2(b^2 + c^2 - a^2) + b^2(a^2 + c^2 - b^2) + c^2(a^2 + b^2 - c^2))}$$ where $(x_1, y_1)$, $(x_2, y_2)$, and $(x_3, y_3)$ are the coordinates of the three points, and $a$, $b$, and $c$ are the lengths of the sides of the triangle opposite the three points, respectively. Once we have the coordinates of the circumcenter, we can use the distance formula to find the radius of the circle. The distance formula is: $$distance = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$ where $(x_1, y_1)$ and $(x_2, y_2)$ are the coordinates of the two points. Using the inverse determinant and edge length approach we can find the circle that passes through three points. Demonstration:Here is my version of Welzl’s algorithm’s implementation: 1234567891011121314151617181920212223242526272829303132333435363738394041424344export function createWelZelCircle(points) { let minCircle = null; let supportSet = []; if (points.length &gt; 1) { shuffleArray(points); let p = points[0]; let totalPoints = points.length; minCircle = new Circle(p); let index = 1; supportSet.push(p); while (index &lt; totalPoints) { let pi = points[index]; if (!supportSet.some((p) =&gt; p === pi) &amp;&amp; !minCircle.contains(pi)) { let newCircle = updateCircle(supportSet, pi); if (newCircle &amp;&amp; newCircle.radius &gt; minCircle.radius) { minCircle = newCircle; index = 0; continue; } } index++; } } return minCircle;}function updateCircle(supportSet, point) { let updatedCircle = null; const supportSetSize = supportSet.length; switch (supportSetSize) { case 1: updatedCircle = updateCircleWithOnePoint(supportSet, point); break; case 2: updatedCircle = updateCircleWithTwoPoints(supportSet, point); break; case 3: updatedCircle = updateCircleWithThreePoints(supportSet, point); break; default: break; } return updatedCircle;} Here is the output: $~$ Overall, my curiosity about how collisions are detected in 3D games led me to discover the minimum enclosing sphere algorithm and its various applications in simulations and other fields. It’s amazing how video games can inspire us to learn and explore new concepts and technologies. To learn about this algorithm more: https://people.inf.ethz.ch/emo/PublFiles/SmallEnclDisk_LNCS555_91.pdf","link":"/2023/05/06/welzl-s-algorithm/"}],"tags":[{"name":"design-patterns","slug":"design-patterns","link":"/tags/design-patterns/"},{"name":"system-design","slug":"system-design","link":"/tags/system-design/"},{"name":"multi-threading","slug":"multi-threading","link":"/tags/multi-threading/"},{"name":"misc","slug":"misc","link":"/tags/misc/"},{"name":"trading","slug":"trading","link":"/tags/trading/"},{"name":"technology","slug":"technology","link":"/tags/technology/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"}],"categories":[{"name":"Design Patterns","slug":"design-patterns","link":"/categories/design-patterns/"},{"name":"System Design","slug":"system-design","link":"/categories/system-design/"},{"name":"Operating System","slug":"operating-system","link":"/categories/operating-system/"},{"name":"Technology","slug":"technology","link":"/categories/technology/"},{"name":"Misc","slug":"misc","link":"/categories/misc/"},{"name":"Trading","slug":"trading","link":"/categories/trading/"},{"name":"Algorithms","slug":"algorithms","link":"/categories/algorithms/"}]}